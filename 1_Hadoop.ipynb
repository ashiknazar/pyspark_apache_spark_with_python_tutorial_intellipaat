{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop: An Overview\n",
    "\n",
    "Hadoop is an open-source framework for processing and storing large datasets in a distributed computing environment. It allows for the storage and processing of massive amounts of data across a distributed cluster of computers. Hadoop is particularly well-suited for handling Big Data due to its ability to scale <i>horizontally</i>, its fault-tolerant design, and its use of commodity hardware.\n",
    "\n",
    "## Key Components of Hadoop\n",
    "\n",
    "Hadoop consists of several core components that work together to process and manage large datasets:\n",
    "\n",
    "### 1. **Hadoop Distributed File System (HDFS)**\n",
    "   - **Definition**: HDFS is the storage layer of Hadoop. It is a distributed file system designed to store large files across multiple machines.\n",
    "   - **Features**:\n",
    "     - **Distributed Storage**: Files are split into large blocks (typically 128MB or 256MB) and distributed across multiple nodes in a cluster.\n",
    "     - **Fault Tolerance**: Data is replicated across multiple machines (usually three replicas by default), ensuring reliability even if one node fails.\n",
    "     - **Scalability**: New nodes can be added to the system without disrupting the existing data, allowing Hadoop to scale as needed.\n",
    "   - **Example**: When a user uploads a large file to HDFS, the file is divided into smaller chunks, and these chunks are stored across different nodes in the Hadoop cluster.\n",
    "\n",
    "### 2. **MapReduce**\n",
    "   - **Definition**: MapReduce is a programming model used for processing large datasets in parallel across a Hadoop cluster.\n",
    "   - **How It Works**:\n",
    "     - **Map Phase**: The input data is divided into smaller chunks, and a \"map\" function processes each chunk independently. Each map function emits key-value pairs.\n",
    "     - **Reduce Phase**: The key-value pairs are shuffled and sorted by key, and then the \"reduce\" function aggregates the data based on the keys.\n",
    "   - **Example**: If you were analyzing the frequency of words in a large document, the map phase would break the text into words, and the reduce phase would sum the occurrences of each word across the entire dataset.\n",
    "\n",
    "### 3. **YARN (Yet Another Resource Negotiator)**\n",
    "   - **Definition**: YARN is the resource management layer of Hadoop. It manages and schedules resources across the cluster, ensuring that jobs are efficiently allocated and executed.\n",
    "   - **Features**:\n",
    "     - **Resource Allocation**: YARN allocates resources (memory, CPU) to various applications running in the cluster.\n",
    "     - **Job Scheduling**: YARN schedules the execution of tasks on available nodes, ensuring load balancing and optimal utilization of resources.\n",
    "   - **Example**: If there are multiple applications running on the Hadoop cluster, YARN will ensure that each application gets its required resources without overloading any single node.\n",
    "\n",
    "### 4. **Hadoop Common**\n",
    "   - **Definition**: Hadoop Common is a collection of libraries and utilities required by other Hadoop modules. It provides the necessary Java libraries and APIs to support the functionality of HDFS, MapReduce, and YARN.\n",
    "   - **Example**: Libraries in Hadoop Common provide the necessary file system access, serialization, and network communication utilities required by other Hadoop components.\n",
    "\n",
    "## Advantages of Hadoop\n",
    "\n",
    "1. **Scalability**: Hadoop is designed to scale horizontally, meaning that as data grows, you can simply add more nodes to the cluster without major reconfiguration or downtime.\n",
    "2. **Cost-Effective**: Hadoop can be run on commodity hardware, making it a cost-effective solution for storing and processing large datasets. It doesn't require expensive, high-end infrastructure.\n",
    "3. **Fault Tolerance**: With data replication and automatic recovery, Hadoop ensures that even in the event of hardware failures, data is not lost and processing can continue without interruption.\n",
    "4. **Flexibility**: Hadoop supports a wide variety of data formats (structured, semi-structured, and unstructured) and processing models. It is suitable for tasks ranging from batch processing to real-time stream processing.\n",
    "5. **Parallel Processing**: Hadoop allows data to be processed in parallel across many machines, significantly speeding up computation and reducing the time it takes to process large datasets.\n",
    "\n",
    "## Use Cases of Hadoop\n",
    "\n",
    "1. **Data Warehousing**: Organizations use Hadoop to create massive data lakes or data warehouses that store structured and unstructured data, which can later be analyzed for business intelligence (BI) insights.\n",
    "2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "- hadoop is a framework which is used to store bigdata across a spectrum of devices\n",
    "- this is done to help you process bigdata in parallel\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yarn keeps track of active nodes (by heart beats)\n",
    "- resiliance (fault tolerance ) is built in hdfs ,not in map reduce\n",
    "\n",
    "___\n",
    "- mapreduce in replaced by spark\n",
    "- spark doesnt have a storage of its own (will be hdfs,s3....)\n",
    "- resource manager can be yarn | own cluster manager |..\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- the problem with map reduce is every proplem is reduced into a key value type. but what if requirement is to read a file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
